{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load environment variables and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "# specify the name of the .env file name \n",
    "env_name = \"example.env\"\n",
    "config = dotenv_values(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_type = config[\"openai_api_type\"] #\"azure\"\n",
    "openai.api_key = config['openai_api_key']\n",
    "openai.api_base = config['openai_api_base'] #\"https://synapseml-openai.openai.azure.com/\"\n",
    "openai.api_version = config['openai_api_version'] \n",
    "openai_deployment = config[\"openai_deployment_embedding\"]\n",
    "EMBEDDING_LENGTH = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "cogsearch_key = config[\"cogsearch_api_key\"]\n",
    "service_endpoint = config[\"cogsearch_endpoint\"]\n",
    "index_name = config[\"cogsearch_index_name\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data to SQL DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database\n",
    "\n",
    "For simplicity, we set `autocommit=True` in the pyodbc connection parameters, which allows us to execute `ALTER` statements. In a production scenario, setting `autocommit=True` is not recommended; instead, the `ALTER` statements can be executed in SSMS or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "# Define Azure SQL database connection details\n",
    "server = config[\"server\"] \n",
    "database = config[\"database\"] \n",
    "username = config[\"username\"] \n",
    "password = config[\"password\"] \n",
    "driver = '{ODBC Driver 18 for SQL Server}'\n",
    "\n",
    "# Create a connection string\n",
    "conn_str = f\"DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "\n",
    "# Establish a connection to the Azure SQL database\n",
    "conn = pyodbc.connect(conn_str, autocommit=True)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a table in the database\n",
    "\n",
    "We will create a new table \"foodreview\" and upload the data from a csv file. We include a primary key, which is necessary for change tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished dropping table (if existed)\n",
      "Finished creating table\n",
      "Finished creating index\n"
     ]
    }
   ],
   "source": [
    "table_name = \"foodreview\" \n",
    "\n",
    "# Drop previous table of same name if one exists\n",
    "cursor.execute(f\"DROP TABLE IF EXISTS {table_name};\")\n",
    "print(\"Finished dropping table (if existed)\")\n",
    "\n",
    "# Create a table\n",
    "cursor.execute(f\"\"\"\n",
    "               CREATE TABLE {table_name} \n",
    "               (Id int NOT NULL, \n",
    "               CONSTRAINT PK_{table_name}_Id PRIMARY KEY CLUSTERED (Id), \n",
    "               ProductId text, \n",
    "               UserId text, \n",
    "               ProfileName text, \n",
    "               HelpfulnessNumerator integer, \n",
    "               HelpfulnessDenominator integer, \n",
    "               Score integer, \n",
    "               Time bigint, \n",
    "               Summary text, \n",
    "               Text text);\n",
    "               \"\"\")\n",
    "print(\"Finished creating table\")\n",
    "\n",
    "# Create a index\n",
    "cursor.execute(f\"CREATE INDEX idx_Id ON {table_name}(Id);\")\n",
    "print(\"Finished creating index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable change tracking\n",
    "\n",
    "This allows the us to automatically update the index when changes are made to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('42000', \"[42000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Change tracking is already enabled for database 'test_vector_notebook'. (5088) (SQLExecDirectW); [42000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]ALTER DATABASE statement failed. (5069)\")\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cursor.execute(f\"ALTER DATABASE {database} SET CHANGE_TRACKING = ON (CHANGE_RETENTION = 2 DAYS, AUTO_CLEANUP = ON)\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cursor.execute(f\"ALTER TABLE {table_name} ENABLE CHANGE_TRACKING WITH (TRACK_COLUMNS_UPDATED = ON)\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data\n",
    "\n",
    "We split our data into an initial dataset and an extra dataset (the extra dataset will be used demo the change tracking feature later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_all = pd.read_csv('../../DataSet/Reviews_small.csv')\n",
    "\n",
    "# Split data into 'initial' and 'extra' data\n",
    "df_initial = df_all[:50].copy()\n",
    "df_extra = df_all[50:].copy().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload initial data\n",
    "df = df_initial \n",
    "\n",
    "# Specify the batch size\n",
    "batch_size = 30\n",
    "\n",
    "# Split the dataframe into batches\n",
    "batches = [df[i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "#Iterate over each batch and insert the data into the database\n",
    "for batch in batches:\n",
    "    # Convert the batch dataframe to a list of tuples for bulk insertion\n",
    "    rows = [tuple(row) for row in batch.itertuples(index=False)]\n",
    "    \n",
    "    # Define the SQL query for bulk insertion\n",
    "    query = f\"INSERT INTO {table_name} (Id, ProductId, UserId, ProfileName, HelpfulnessNumerator, HelpfulnessDenominator, Score, Time, Summary, Text) \\\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "    cursor.executemany(query, rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, )\n"
     ]
    }
   ],
   "source": [
    "# Execute the SELECT statement\n",
    "try:\n",
    "    cursor.execute(f\"SELECT count(Id) FROM {table_name};\")\n",
    "    rows = cursor.fetchall()\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "except Exception as e:\n",
    "    print(f\"Error executing SELECT statement: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commit changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.commit()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up data source connection in Cog Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import needed CogSearch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient  \n",
    "from azure.search.documents.models import (\n",
    "    QueryAnswerType,\n",
    "    QueryCaptionType,\n",
    "    QueryLanguage,\n",
    "    QueryType,\n",
    "    RawVectorQuery,\n",
    "    VectorizableTextQuery,\n",
    "    VectorFilterMode,    \n",
    ")\n",
    "from azure.search.documents.indexes.models import (  \n",
    "    AzureOpenAIEmbeddingSkill,  \n",
    "    AzureOpenAIParameters,  \n",
    "    AzureOpenAIVectorizer,  \n",
    "    ExhaustiveKnnParameters,  \n",
    "    ExhaustiveKnnVectorSearchAlgorithmConfiguration,\n",
    "    FieldMapping,  \n",
    "    HnswParameters,  \n",
    "    HnswVectorSearchAlgorithmConfiguration,  \n",
    "    IndexProjectionMode,  \n",
    "    InputFieldMappingEntry,  \n",
    "    OutputFieldMappingEntry,  \n",
    "    PrioritizedFields,    \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SearchIndex,  \n",
    "    SearchIndexer,  \n",
    "    SearchIndexerDataContainer,  \n",
    "    SearchIndexerDataSourceConnection,  \n",
    "    SearchIndexerIndexProjectionSelector,  \n",
    "    SearchIndexerIndexProjections,  \n",
    "    SearchIndexerIndexProjectionsParameters,  \n",
    "    SearchIndexerSkillset,  \n",
    "    SemanticConfiguration,  \n",
    "    SemanticField,  \n",
    "    SemanticSettings,  \n",
    "    SplitSkill,  \n",
    "    SqlIntegratedChangeTrackingPolicy,\n",
    "    VectorSearch,  \n",
    "    VectorSearchAlgorithmKind,  \n",
    "    VectorSearchAlgorithmMetric,  \n",
    "    VectorSearchProfile,  \n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data source connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source 'amazon-review-jordan-v1-azuresql-connection' created or updated\n"
     ]
    }
   ],
   "source": [
    "ds_conn_str = f'Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;Server=tcp:{server};Database={database};User ID={username};Password={password};'\n",
    "\n",
    "cogsearch_credential = AzureKeyCredential(cogsearch_key)\n",
    "ds_client = SearchIndexerClient(service_endpoint, cogsearch_credential)\n",
    "container = SearchIndexerDataContainer(name=table_name)\n",
    "#\"#Microsoft.Azure.Search.SqlIntegratedChangeTrackingPolicy\"\n",
    "change_detection_policy = SqlIntegratedChangeTrackingPolicy()\n",
    "\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=f\"{index_name}-azuresql-connection\",\n",
    "    type=\"azuresql\",\n",
    "    connection_string=ds_conn_str,\n",
    "    container=container,\n",
    "    data_change_detection_policy=change_detection_policy\n",
    ")\n",
    "data_source = ds_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "print(f\"Data source '{data_source.name}' created or updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up automatic indexing + vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon-review-jordan-v1 created\n"
     ]
    }
   ],
   "source": [
    "# Create a search index\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=cogsearch_credential)\n",
    "\n",
    "fields = [\n",
    "    SearchField(name=\"Id\", type=SearchFieldDataType.String, key=True,\n",
    "                sortable=True, filterable=True, facetable=True, analyzer_name=\"keyword\"),\n",
    "    SearchField(name=\"parent_id\", type=SearchFieldDataType.String, sortable=True, filterable=True, facetable=True),\n",
    "    SearchField(name=\"chunk\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),\n",
    "    SearchField(name=\"vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), \n",
    "                vector_search_dimensions=EMBEDDING_LENGTH, vector_search_profile=\"my-vector-search-profile\")\n",
    "]\n",
    "\n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswVectorSearchAlgorithmConfiguration(\n",
    "            name=\"my-hnsw-config\",\n",
    "            kind=VectorSearchAlgorithmKind.HNSW\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"my-vector-search-profile\",\n",
    "            algorithm=\"my-hnsw-config\",\n",
    "            vectorizer=\"my-openai\"\n",
    "        )\n",
    "    ],\n",
    "    vectorizers=[\n",
    "        AzureOpenAIVectorizer(\n",
    "            name=\"my-openai\",\n",
    "            kind=\"azureOpenAI\",\n",
    "            azure_open_ai_parameters=AzureOpenAIParameters(\n",
    "                resource_uri=openai.api_base,\n",
    "                deployment_id=openai_deployment,\n",
    "                api_key=openai.api_key\n",
    "            )\n",
    "        )  \n",
    "    ]  \n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"Id\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields,\n",
    "                    vector_search=vector_search, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f'{result.name} created')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create skillset\n",
    "\n",
    "This section of the code sets up the automatic chunking of text and automatic vectorization of the text after chunking. We recommend the following resources to learn more about the process and how one can adapt it to different applications:\n",
    "* [Overview of indexers](https://learn.microsoft.com/en-us/azure/search/search-indexer-overview)\n",
    "* [Skill context and input annotation language](https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-annotation-language)\n",
    "* [Reference inputs and outputs in skillsets](https://learn.microsoft.com/en-us/azure/search/cognitive-search-concept-annotations-syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " amazon-review-jordan-v1-skillset created\n"
     ]
    }
   ],
   "source": [
    "# Create a skillset  \n",
    "skillset_name = f\"{index_name}-skillset\"  \n",
    "\n",
    "split_skill = SplitSkill(  \n",
    "    description=\"Split skill to chunk documents\",  \n",
    "    text_split_mode=\"pages\",  \n",
    "    context=\"/document\",  \n",
    "    maximum_page_length=300,  \n",
    "    page_overlap_length=20,  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/Text\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"textItems\", target_name=\"pages\")  \n",
    "    ]  \n",
    ")\n",
    "\n",
    "embedding_skill = AzureOpenAIEmbeddingSkill(  \n",
    "    description=\"Skill to generate embeddings via Azure OpenAI\",  \n",
    "    context=\"/document/pages/*\",  \n",
    "    resource_uri=openai.api_base,  \n",
    "    deployment_id=openai_deployment,  \n",
    "    api_key=openai.api_key,  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"embedding\", target_name=\"vector\")  \n",
    "    ]  \n",
    ")  \n",
    "\n",
    "index_projections = SearchIndexerIndexProjections(  \n",
    "    selectors=[  \n",
    "        SearchIndexerIndexProjectionSelector(  \n",
    "            target_index_name=index_name,  \n",
    "            parent_key_field_name=\"parent_id\",  \n",
    "            source_context=\"/document/pages/*\",  \n",
    "            mappings=[  \n",
    "                InputFieldMappingEntry(name=\"chunk\", source=\"/document/pages/*\"),\n",
    "                InputFieldMappingEntry(name=\"vector\", source=\"/document/pages/*/vector\")\n",
    "            ],  \n",
    "        ),  \n",
    "    ],  \n",
    "    parameters=SearchIndexerIndexProjectionsParameters(  \n",
    "        projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS  \n",
    "    ),  \n",
    ")  \n",
    "\n",
    "skillset = SearchIndexerSkillset(  \n",
    "    name=skillset_name,  \n",
    "    description=\"Skillset to chunk documents and generating embeddings\",  \n",
    "    skills=[split_skill, embedding_skill],\n",
    "    index_projections=index_projections  \n",
    ")\n",
    "  \n",
    "client = SearchIndexerClient(service_endpoint, cogsearch_credential)  \n",
    "client.create_or_update_skillset(skillset)  \n",
    "print(f' {skillset.name} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create indexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Check connection; make timeout logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " amazon-review-jordan-v1-indexer created\n"
     ]
    }
   ],
   "source": [
    "# Create an indexer  \n",
    "indexer_name = f\"{index_name}-indexer\"  \n",
    "\n",
    "indexer = SearchIndexer(  \n",
    "    name=indexer_name,  \n",
    "    description=\"Indexer to chunk documents and generate embeddings\",  \n",
    "    skillset_name=skillset_name,  \n",
    "    target_index_name=index_name,  \n",
    "    data_source_name=data_source.name\n",
    ")  \n",
    "  \n",
    "indexer_client = SearchIndexerClient(service_endpoint, cogsearch_credential)\n",
    "indexer_result = indexer_client.create_or_update_indexer(indexer)  \n",
    "\n",
    "# Run the indexer  \n",
    "indexer_client.run_indexer(indexer_name)\n",
    "print(f' {indexer_name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexer status: running\n"
     ]
    }
   ],
   "source": [
    "# Get the status of the indexer  \n",
    "indexer_status = indexer_client.get_indexer_status(indexer_name)\n",
    "print(f\"Indexer status: {indexer_status.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow some time for the indexer to process the data\n",
    "import time\n",
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use vector search for sample application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Canned dog food\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following output, we find the top 3 chunks that are most relevant to the user's query.\n",
    "\n",
    "Feel free to retry the following cell in case of an empty response or a 429 error. An empty response probably indicates that the chunking/embedding process has not finished yet. A 429 error means there have been too many requests to the OpenAI embedding service and should go away on retrying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk id: 7f37f895690c_1_pages_0\n",
      "Parent Id: 1\n",
      "Search score: 0.880716\n",
      "Text chunk: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
      "-----\n",
      "Chunk id: 675e8e92bf89_10_pages_0\n",
      "Parent Id: 10\n",
      "Search score: 0.84844846\n",
      "Text chunk: This is a very healthy dog food. Good for their digestion. Also good for small puppies. My dog eats her required amount at every feeding.\n",
      "-----\n",
      "Chunk id: 98a114de2791_33_pages_2\n",
      "Parent Id: 33\n",
      "Search score: 0.8391927\n",
      "Text chunk: pack taste good.  It can be prepared in the microwave or by adding boiling water so it is convenient in the extreme when time is an issue.<br /><br />McCann's use of actual cane sugar instead of high fructose corn syrup helped me decide to buy this product.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "search_client = SearchClient(service_endpoint, index_name, credential=cogsearch_credential)\n",
    "vector_query = VectorizableTextQuery(text=user_query, k=3, fields=\"vector\", exhaustive=True)\n",
    "# Use the query below to pass in the raw vector query instead of the query vectorization\n",
    "# vector_query = RawVectorQuery(vector=generate_embeddings(user_query), k=3, fields=\"vector\")\n",
    "  \n",
    "results = search_client.search(\n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"Id\", \"parent_id\", \"chunk\"],\n",
    "    top=3\n",
    ")\n",
    "\n",
    "for result in results: \n",
    "    print(f\"Chunk id: {result['Id']}\")\n",
    "    print(f\"Parent Id: {result['parent_id']}\")\n",
    "    print(f\"Search score: {result['@search.score']}\")\n",
    "    print(f\"Text chunk: {result['chunk']}\")\n",
    "    print(\"-----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate GPT Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt template \n",
    "template = \"\"\"\n",
    "    The user's question is: {query}\n",
    "    The context for this question is:{context}\n",
    "    Answer the question based on the provided context. Your answer should summarize one or more relevant\n",
    "    reviews, and be sure to mention the corresponding product id's.\n",
    "    Question: {query}\n",
    "    Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These three product reviews are relevant to the user's question: \n",
      "Product id: 1. Review text: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
      "Product id: 10. Review text: This is a very healthy dog food. Good for their digestion. Also good for small puppies. My dog eats her required amount at every feeding.\n",
      "Product id: 33. Review text: pack taste good.  It can be prepared in the microwave or by adding boiling water so it is convenient in the extreme when time is an issue.<br /><br />McCann's use of actual cane sugar instead of high fructose corn syrup helped me decide to buy this product.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create the context from the search response (requires regenerating results)\n",
    "results = search_client.search(\n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"Id\", \"parent_id\", \"chunk\"],\n",
    "    top=3\n",
    ")\n",
    "\n",
    "context = \"These three product reviews are relevant to the user's question: \\n\"\n",
    "\n",
    "for result in results:\n",
    "    context += f\"Product id: {result['parent_id']}. Review text: {result['chunk']}\"\n",
    "    context += \"\\n\"\n",
    "    \n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The user's question is: Canned dog food\n",
      "    The context for this question is:These three product reviews are relevant to the user's question: \n",
      "Product id: 1. Review text: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
      "Product id: 10. Review text: This is a very healthy dog food. Good for their digestion. Also good for small puppies. My dog eats her required amount at every feeding.\n",
      "Product id: 33. Review text: pack taste good.  It can be prepared in the microwave or by adding boiling water so it is convenient in the extreme when time is an issue.<br /><br />McCann's use of actual cane sugar instead of high fructose corn syrup helped me decide to buy this product.\n",
      "\n",
      "    Answer the question based on the provided context. Your answer should summarize one or more relevant\n",
      "    reviews, and be sure to mention the corresponding product id's.\n",
      "    Question: Canned dog food\n",
      "    Answer: \n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(context=context, query=user_query)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Product id 1 and 10 both show that the canned dog food they have bought is of high quality and provides a healthy meal for their dogs. Product id 33 mentions McCann's use of actual cane sugar instead of high fructose corn syrup, which makes the product attractive to buyers.\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "    engine= config[\"openai_deployment_completion\"],\n",
    "    prompt=prompt,\n",
    "    max_tokens=1024,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=1,\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
